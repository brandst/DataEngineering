{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3bf0613-d6ec-4cf4-87e5-062fd3bd3a82",
   "metadata": {},
   "source": [
    "### Installation\n",
    "Install the packages required for executing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69f1d825-84cc-43ac-9fe2-f204d77f0962",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp>2 in ./.local/lib/python3.10/site-packages (2.7.0)\n",
      "Collecting kfp>2\n",
      "  Downloading kfp-2.9.0.tar.gz (595 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m595.6/595.6 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: google-cloud-pipeline-components>2 in ./.local/lib/python3.10/site-packages (2.17.0)\n",
      "Requirement already satisfied: google-cloud-aiplatform in ./.local/lib/python3.10/site-packages (1.69.0)\n",
      "Requirement already satisfied: click<9,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (8.1.7)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (0.16)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in ./.local/lib/python3.10/site-packages (from kfp>2) (2.20.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (2.35.0)\n",
      "Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (2.14.0)\n",
      "Collecting kfp-pipeline-spec==0.4.0 (from kfp>2)\n",
      "  Downloading kfp_pipeline_spec-0.4.0-py3-none-any.whl.metadata (301 bytes)\n",
      "Collecting kfp-server-api<2.4.0,>=2.1.0 (from kfp>2)\n",
      "  Downloading kfp_server_api-2.3.0.tar.gz (84 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: kubernetes<31,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (26.1.0)\n",
      "Requirement already satisfied: protobuf<5,>=4.21.1 in ./.local/lib/python3.10/site-packages (from kfp>2) (4.25.5)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (6.0.2)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (0.10.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (1.26.20)\n",
      "Requirement already satisfied: Jinja2<4,>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-pipeline-components>2) (3.1.4)\n",
      "Requirement already satisfied: kfp-pipeline-spec==0.3.0 in ./.local/lib/python3.10/site-packages (from kfp>2) (0.3.0)\n",
      "Requirement already satisfied: kfp-server-api<2.1.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (2.0.5)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.24.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (24.1)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.25.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.12.5)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.0.6)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.10.18)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (1.65.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.66.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp>2) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp>2) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp>2) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.2)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.9.0.post0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3,>=2.2.1->kfp>2) (1.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2<4,>=3.1.2->google-cloud-pipeline-components>2) (2.1.5)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp>2) (1.16.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp>2) (2024.8.30)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<31,>=8.0.0->kfp>2) (73.0.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<31,>=8.0.0->kfp>2) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes<31,>=8.0.0->kfp>2) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (4.12.2)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /opt/conda/lib/python3.10/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.25.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp>2) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (3.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib->kubernetes<31,>=8.0.0->kfp>2) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "# Install the packages\n",
    "! pip3 install --user --no-cache-dir --upgrade \"kfp>2\" \"google-cloud-pipeline-components>2\" \\\n",
    "                                        google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bc6a21-604f-4a52-b904-e3bb18a61b2f",
   "metadata": {},
   "source": [
    "## Restart the kernel\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52dad0c4-c173-46b8-bf99-d6e8efc35316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2207b06-771f-4dbb-a713-90c50745c0ea",
   "metadata": {},
   "source": [
    "## Check the versions of the packages you installed. The KFP SDK version should be >2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5b60838-e5a2-41cd-ae93-43925343fba5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 2.7.0\n",
      "google-cloud-aiplatform==1.69.0\n",
      "google_cloud_pipeline_components version: 2.17.0\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! pip3 freeze | grep aiplatform\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f0bcff2-3ffb-4e51-b852-511cb10ad0f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "import typing\n",
    "from typing import Dict\n",
    "from typing import NamedTuple\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "import google.cloud.aiplatform as aip\n",
    "from google_cloud_pipeline_components.types import artifact_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01afffb0-449b-4669-807a-793f526277fe",
   "metadata": {},
   "source": [
    "#### Project and Pipeline Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abf6aad4-f675-47aa-820b-14daa796d89f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#The Google Cloud project that this pipeline runs in.\n",
    "PROJECT_ID = \"your poject id\"\n",
    "# The region that this pipeline runs in\n",
    "REGION = \"us-central1\"\n",
    "# Specify a Cloud Storage URI that your pipelines service account can access. The artifacts of your pipeline runs are stored within the pipeline root.\n",
    "PIPELINE_ROOT = \"gs://your temp bucket\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2457ef88-cd95-4304-b6e0-143b718c44aa",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83bc305f-2456-4c07-b89f-427b0f24eaf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\",\"google-cloud-storage\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def download_data(project_id: str, bucket: str, file_name: str, dataset: Output[Dataset]):\n",
    "    '''download data'''\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    # Downloaing the file from a google bucket \n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(bucket)\n",
    "    blob = bucket.blob(file_name)\n",
    "    blob.download_to_filename(dataset.path + \".csv\")\n",
    "    logging.info('Downloaded Data!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3494d2-bea7-415f-9832-fcf1f2c9fe4a",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Training-MLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e194fc48-1343-4bd5-9c69-f3d1ce826321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import logging\n",
    "import sys\n",
    "from typing import NamedTuple\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the DSL component decorator\n",
    "@dsl.component(\n",
    "    packages_to_install=['pandas', 'keras', 'tensorflow', 'h5py'],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_mlp(features: Input[Dataset], out_model: Output[Model]) -> NamedTuple('outputs', metrics=dict):\n",
    "    '''Train a Multi-Layer Perceptron (MLP) with cross-validation and save the model.'''\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(features.path + \".csv\")\n",
    "    logging.info(\"Columns: %s\", df.columns)\n",
    "\n",
    "    # Split features and target\n",
    "    X = df.iloc[:, :-1]  # All columns except the last\n",
    "    y = df.iloc[:, -1]   # The last column\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Initialize StratifiedKFold for cross-validation\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Collect results for accuracy\n",
    "    accuracies = []\n",
    "\n",
    "    # Cross-validation loop\n",
    "    for train_index, val_index in kf.split(X_train, y_train):\n",
    "        X_fold_train, X_fold_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_fold_train, y_fold_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "        # Standardize the data\n",
    "        scaler = StandardScaler()\n",
    "        X_fold_train_scaled = scaler.fit_transform(X_fold_train)\n",
    "        X_fold_val_scaled = scaler.transform(X_fold_val)\n",
    "\n",
    "        # Build the MLP model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_dim=X_fold_train_scaled.shape[1], activation='relu', kernel_regularizer=l2(0.01)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.01)))\n",
    "        model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "\n",
    "        # Compile the model\n",
    "        optimizer = Adam(learning_rate=0.001)\n",
    "        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        # Early stopping callback\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(X_fold_train_scaled, y_fold_train,\n",
    "                            epochs=200,\n",
    "                            batch_size=16,\n",
    "                            validation_data=(X_fold_val_scaled, y_fold_val),\n",
    "                            callbacks=[early_stopping],\n",
    "                            verbose=0)\n",
    "\n",
    "        # Evaluate the model on validation set\n",
    "        val_loss, val_acc = model.evaluate(X_fold_val_scaled, y_fold_val, verbose=0)\n",
    "        accuracies.append(val_acc)\n",
    "\n",
    "    # Final evaluation on the test set\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Retrain the model on the entire training data\n",
    "    model.fit(X_train_scaled, y_train, epochs=200, batch_size=16, verbose=0)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_acc = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "\n",
    "    # Save the model\n",
    "    try:\n",
    "        model_repo = os.environ['MODEL_REPO']\n",
    "        if model_repo:\n",
    "            file_path = os.path.join(model_repo, \"heart_disease_model.h5\")\n",
    "            model.save(file_path)\n",
    "            logging.info(\"Saved the model to the location: \" + model_repo)\n",
    "        else:\n",
    "            logging.error(\"MODEL_REPO environment variable is not set.\")\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error saving the model: %s\", e)\n",
    "\n",
    "    # Return metrics as NamedTuple\n",
    "    metrics_dict = {\n",
    "        \"accuracy\": test_acc,\n",
    "        \"loss\": test_loss,\n",
    "    }\n",
    "\n",
    "    # Define metadata for the output model\n",
    "    out_model.metadata[\"file_type\"] = \".h5\"\n",
    "    out_model.metadata[\"algo\"] = \"mlp\"\n",
    "\n",
    "    return NamedTuple('outputs', metrics=dict)(metrics=metrics_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f911d312-549c-4be7-bef0-e02c9d8cf80f",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Training LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acee72f0-007a-4e9c-853a-e6cf66f2a4fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['pandas', 'scikit-learn==1.3.2'],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_lr(features: Input[Dataset], out_model: Output[Model]) -> NamedTuple('outputs', metrics=dict):\n",
    "    '''Train a Logistic Regression model with default parameters'''\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import logging \n",
    "    import sys\n",
    "    import os\n",
    "    import pickle  \n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(features.path + \".csv\")\n",
    "    logging.info(\"Columns: %s\", df.columns)        \n",
    "\n",
    "    # Create a copy of the DataFrame\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Split features and target\n",
    "    X = df_copy.drop('target', axis=1)  # Use 'target' column as the label\n",
    "    y = df_copy['target']\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)\n",
    "\n",
    "    # Initialize and train the Logistic Regression model\n",
    "    model_lr = LogisticRegression(max_iter=150)  # Increase max_iter if needed\n",
    "    model_lr.fit(x_train, y_train)\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics_dict = {\n",
    "        \"accuracy\": model_lr.score(x_test, y_test)\n",
    "    }\n",
    "    logging.info(\"Metrics: %s\", metrics_dict)  \n",
    "\n",
    "    # Save the model\n",
    "    model_file = out_model.path + \".pkl\"\n",
    "    with open(model_file, 'wb') as f:  \n",
    "        pickle.dump(model_lr, f)\n",
    "\n",
    "    # Define metadata for the output model\n",
    "    out_model.metadata[\"file_type\"] = \".pkl\"\n",
    "    out_model.metadata[\"algo\"] = \"lr\"\n",
    "\n",
    "    # Return metrics as NamedTuple\n",
    "    return NamedTuple('outputs', metrics=dict)(metrics_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33b9f62-14d3-4b97-bd9a-f15b3d47758a",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Prediction-MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa57d2d1-2a2b-4c43-8a3b-1399471e782c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['pandas', 'keras', 'tensorflow', 'h5py', 'joblib'],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def predict_mlp(model: Input[Model], features: Input[Dataset], results: Output[Dataset]):\n",
    "    import pandas as pd\n",
    "    from keras.models import load_model\n",
    "    import logging\n",
    "    import sys\n",
    "    import os\n",
    "    import joblib\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    # Load the features dataset\n",
    "    df = pd.read_csv(features.path + \".csv\")\n",
    "    logging.info(\"Columns: %s\", df.columns)\n",
    "\n",
    "    # Create a copy of the DataFrame for modifications\n",
    "    dfcp = df.copy()\n",
    "\n",
    "    # Extract features for prediction\n",
    "    xNew = dfcp.iloc[:, :-1].values  # Use all columns except the last one\n",
    "    logging.info(\"New features shape: %s\", xNew.shape)\n",
    "\n",
    "    # Load the model\n",
    "    model_mlp = load_model(model.path + '.h5')\n",
    "\n",
    "    # Load the scaler\n",
    "    scaler_file_path = os.path.join(model.path, \"scaler.pkl\") \n",
    "    scaler = joblib.load(scaler_file_path)\n",
    "\n",
    "    # Standardize the new input features\n",
    "    xNew_scaled = scaler.transform(xNew)\n",
    "\n",
    "    # Make predictions\n",
    "    result = model_mlp.predict(xNew_scaled)\n",
    "    y_classes = (result > 0.5).astype(int)  \n",
    "    logging.info(\"Predicted classes: %s\", y_classes)\n",
    "\n",
    "    # Prepare results\n",
    "    dfcp['predicted_class'] = y_classes.tolist()\n",
    "    dfcp.to_csv(results.path + \".csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f4e9c1-a1d4-412f-8533-8e741fd3f79c",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Prediction-LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e70377d6-f3f8-4320-9689-9e17a695674a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['pandas', 'scikit-learn==1.3.2'],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def predict_lr(model: Input[Model], features: Input[Dataset], results: Output[Dataset]):\n",
    "    import pandas as pd\n",
    "    import pickle  \n",
    "    import logging\n",
    "    import sys\n",
    "    import os\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    # Load the features dataset\n",
    "    df = pd.read_csv(features.path + \".csv\")\n",
    "    logging.info(\"Columns: %s\", df.columns)\n",
    "\n",
    "    # Create a copy of the DataFrame\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Load the saved model\n",
    "    model_file = model.path + \".pkl\"\n",
    "    model_lr = pickle.load(open(model_file, 'rb'))\n",
    "\n",
    "    # Extract features for prediction (using the same features as in training)\n",
    "    xNew = df_copy.drop('target', axis=1)  # Exclude 'target' column from features\n",
    "\n",
    "    # Make predictions\n",
    "    y_classes = model_lr.predict(xNew)\n",
    "    logging.info(\"Predicted classes: %s\", y_classes)\n",
    "\n",
    "    # Prepare results\n",
    "    df_copy['predicted_class'] = y_classes.tolist()  # Add predictions to the DataFrame\n",
    "    df_copy.to_csv(results.path + \".csv\", index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c183e7-f9dd-48d7-b37e-69646bb08203",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Algorithm Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "752057b5-8389-4370-97d5-19e30ebb8dd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def compare_model(mlp_metrics: dict, lr_metrics: dict) -> str:\n",
    "    import logging\n",
    "    import json\n",
    "    import sys\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    logging.info(mlp_metrics)\n",
    "    logging.info(lr_metrics)\n",
    "    if mlp_metrics.get(\"accuracy\") > lr_metrics.get(\"accuracy\"):\n",
    "        return \"MLP\"\n",
    "    else :\n",
    "        return \"LR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ed599-9cd0-4215-a17d-1a8baaa2c76c",
   "metadata": {},
   "source": [
    "### Upload Model and Metrics to Google Bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9344f481-88ca-4220-a0af-3a055c61455b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"google-cloud-storage\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def upload_model_to_gcs(project_id: str, model_repo: str, model: Input[Model]):\n",
    "    '''upload model to gsc'''\n",
    "    from google.cloud import storage   \n",
    "    import logging \n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)    \n",
    "  \n",
    "    # upload the model to GCS\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(model_repo)\n",
    "    blob = bucket.blob(str(model.metadata[\"algo\"]) + '_model' + str(model.metadata[\"file_type\"])) \n",
    "    blob.upload_from_filename(model.path + str(model.metadata[\"file_type\"]))       \n",
    "    \n",
    "    print(\"Saved the model to GCP bucket : \" + model_repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166590b3-f788-4e4c-8e31-fb981da56966",
   "metadata": {},
   "source": [
    "#### Define the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a96b6ae0-234b-4883-ae95-8599689a5e07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the workflow of the pipeline.\n",
    "@kfp.dsl.pipeline(\n",
    "    name=\"heart_disease_prediction_pipeline\",)\n",
    "def pipeline(project_id: str, data_bucket: str, trainset_filename: str, model_repo: str, testset_filename: str):\n",
    "    \n",
    "    \n",
    "    di_op = download_data(\n",
    "        project_id=project_id,\n",
    "        bucket=data_bucket,\n",
    "        file_name=trainset_filename\n",
    "    )\n",
    "\n",
    " \n",
    "    training_mlp_job_run_op = train_mlp(\n",
    "        features=di_op.outputs[\"dataset\"]\n",
    "    )\n",
    "    \n",
    "     \n",
    "    training_lr_job_run_op = train_lr(\n",
    "        features=di_op.outputs[\"dataset\"]\n",
    "    )\n",
    "    \n",
    "    pre_di_op = download_data(\n",
    "        project_id=project_id,\n",
    "        bucket=data_bucket,\n",
    "        file_name=testset_filename\n",
    "    ).after(training_mlp_job_run_op, training_lr_job_run_op)\n",
    "        \n",
    "        \n",
    "    comp_model__op = compare_model(mlp_metrics=training_mlp_job_run_op.outputs[\"metrics\"],\n",
    "                                       lr_metrics=training_lr_job_run_op.outputs[\"metrics\"]).after(training_mlp_job_run_op, training_lr_job_run_op)  \n",
    "    \n",
    "    # defining the branching condition\n",
    "    with dsl.If(comp_model__op.output==\"MLP\"):\n",
    "        predict_mlp_job_run_op = predict_mlp(\n",
    "            model=training_mlp_job_run_op.outputs[\"out_model\"],      \n",
    "            features=pre_di_op.outputs[\"dataset\"]\n",
    "        )\n",
    "        upload_model_mlp_to_gc_op = upload_model_to_gcs(\n",
    "            project_id=project_id,\n",
    "            model_repo=model_repo,\n",
    "            model=training_mlp_job_run_op.outputs['out_model']\n",
    "        ).after(predict_mlp_job_run_op)\n",
    "        \n",
    "    with dsl.If(comp_model__op.output==\"LR\"):\n",
    "        predict_lr_job_run_op = predict_lr(\n",
    "            model=training_lr_job_run_op.outputs[\"out_model\"],     \n",
    "            features=pre_di_op.outputs[\"dataset\"]\n",
    "        )\n",
    "        upload_model_lr_to_gc_op = upload_model_to_gcs(\n",
    "            project_id=project_id,\n",
    "            model_repo=model_repo,\n",
    "            model=training_lr_job_run_op.outputs['out_model']\n",
    "        ).after(predict_lr_job_run_op)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac278200-c580-4f40-bc8b-1817d3b13c13",
   "metadata": {},
   "source": [
    "#### Compile the pipeline into a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8ee4b21-89e6-4f63-845c-b249556ea919",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='heart_disease_predictor_pipeline.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f87025e-08d7-4608-b37d-c929b6eb5a3c",
   "metadata": {},
   "source": [
    "#### Submit the pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83b88e89-42cd-4e64-bc4e-8e3eddebccff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/966204339179/locations/us-central1/pipelineJobs/diabetes-prdictor-training-pipeline-v2-20241007202913\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/966204339179/locations/us-central1/pipelineJobs/diabetes-prdictor-training-pipeline-v2-20241007202913')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/diabetes-prdictor-training-pipeline-v2-20241007202913?project=966204339179\n",
      "PipelineJob projects/966204339179/locations/us-central1/pipelineJobs/diabetes-prdictor-training-pipeline-v2-20241007202913 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/966204339179/locations/us-central1/pipelineJobs/diabetes-prdictor-training-pipeline-v2-20241007202913 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/966204339179/locations/us-central1/pipelineJobs/diabetes-prdictor-training-pipeline-v2-20241007202913 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/966204339179/locations/us-central1/pipelineJobs/diabetes-prdictor-training-pipeline-v2-20241007202913 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/966204339179/locations/us-central1/pipelineJobs/diabetes-prdictor-training-pipeline-v2-20241007202913 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/966204339179/locations/us-central1/pipelineJobs/diabetes-prdictor-training-pipeline-v2-20241007202913 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/966204339179/locations/us-central1/pipelineJobs/diabetes-prdictor-training-pipeline-v2-20241007202913\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "# Before initializing, make sure to set the GOOGLE_APPLICATION_CREDENTIALS\n",
    "# environment variable to the path of your service account.\n",
    "aip.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")\n",
    "\n",
    "# Prepare the pipeline job\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"heart_disease_prediction_pipeline\",\n",
    "    enable_caching=False,\n",
    "    template_path=\"heart_disease_predictor_pipeline.yaml\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    location=REGION,\n",
    "    parameter_values={\n",
    "        'project_id': PROJECT_ID, # makesure to use your project id \n",
    "        'data_bucket': 'heart_pred_data_bucket',  # makesure to use your data bucket name \n",
    "        'trainset_filename': 'training_set.csv',     # makesure to upload these to your data bucket \n",
    "        'testset_filename': 'test_set.csv',    # makesure to upload these to your data bucket \n",
    "        'model_repo':'heart_pred_model_bucket' # makesure to use your model bucket name \n",
    "    }\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m84",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m84"
  },
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
